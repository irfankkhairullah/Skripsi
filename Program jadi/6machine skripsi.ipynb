{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sentimen = pd.read_excel(\"hasil klasifikasi.xlsx\")\n",
    "df_sentimen = pd.read_csv(\"xlhomedatatahun2018_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentimen['Text']= df_sentimen['Text'].str.lower()\n",
    "df = df_sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, str(input_txt))\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', str(input_txt))\n",
    "    return input_txt    \n",
    "df['remove_user'] = np.vectorize(remove_pattern)(df_sentimen['Text'], \"@[\\w]*\")\n",
    "# def remove(tweet):\n",
    "    # #remove angka\n",
    "    # tweet = re.sub('[0-9]+', '', tweet)\n",
    "    \n",
    "    # # remove stock market tickers like $GE\n",
    "    # tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # # remove old style retweet text \"RT\"\n",
    "    # tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    \n",
    "    # # remove hashtags\n",
    "    # # only removing the hash # sign from the word\n",
    "    # tweet = re.sub(r'#', '', tweet)\n",
    "    # return tweet\n",
    "# df['remove_http'] = df['remove_user'].apply(lambda x: remove(x))\n",
    "# df.drop_duplicates(subset =\"remove_http\", keep = 'first', inplace = True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopword\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_indonesia = stopwords.words('indonesian')\n",
    "# stopwords_eng = stopwords.words('english')\n",
    " \n",
    "#import sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "#tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    #remove coma\n",
    "    tweet = re.sub(r',','',tweet)\n",
    "    \n",
    "    #remove angka\n",
    "    tweet = re.sub('[0-9]+', '', tweet)\n",
    "\n",
    "    #emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    tweet =  emoji_pattern.sub(r'', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (\n",
    "            word not in stopwords_indonesia and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_clean'] = df['remove_user'].apply(lambda x: clean_tweets(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punct\n",
    "def remove_punct(text):\n",
    "    text  = \" \".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "df['Tweet'] = df['tweet_clean'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[[0,1,2,3,4]], axis = 1, inplace = True)\n",
    "df.drop_duplicates(subset =\"Tweet\", keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate To english\n",
    "# !pip install googletrans==3.1.0a0\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from googletrans import LANGUAGES\n",
    "import pandas as pd\n",
    "\n",
    "# headers = ['Tweet', 'translation']\n",
    "# data = pd.read_csv('./target.csv')\n",
    "data = df\n",
    "translator = Translator()\n",
    "\n",
    "def trans(s):\n",
    "  from googletrans import Translator\n",
    "  translator = Translator()\n",
    "  return (translator.translate(s).text)\n",
    "data = data['Tweet'].apply(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['Tweet'].apply(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_tweet = list(data['tweet_clean'])\n",
    "polaritas = 0\n",
    "\n",
    "status = []\n",
    "total_positif = total_negatif = total_netral = total = 0\n",
    "\n",
    "for i, tweet in enumerate(data_tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    polaritas += analysis.polarity\n",
    "\n",
    "    if analysis.sentiment.polarity > 0.0:\n",
    "        total_positif += 1\n",
    "        status.append('Positif')\n",
    "    elif analysis.sentiment.polarity == 0.0:\n",
    "        total_netral += 1\n",
    "        status.append('Netral')\n",
    "    else:\n",
    "        total_negatif += 1\n",
    "        status.append('Negatif')\n",
    "\n",
    "    total += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Hasil Analisis Data:\\nPositif = {total_positif}\\nNetral = {total_netral}\\nNegatif = {total_negatif}')\n",
    "print(f'\\nTotal Data : {total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = pd.DataFrame({'klasifikasi': status})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['klasifikasi'] = status\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_pie(label, data, legend_title) :\n",
    "    fig, ax = plt.subplots(figsize=(8, 10), subplot_kw=dict(aspect='equal'))\n",
    "\n",
    "    labels = [x.split()[-1] for x in label]\n",
    "\n",
    "    def func(pct, allvals):\n",
    "        absolute = int(pct/100.*np.sum(allvals))\n",
    "        return \"{:.1f}% ({:d})\".format(pct, absolute)\n",
    "\n",
    "    wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data), \n",
    "                                      textprops=dict(color=\"w\"))\n",
    "\n",
    "    ax.legend(wedges, labels,\n",
    "              title= legend_title,\n",
    "              loc=\"center left\",\n",
    "              bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "    plt.setp(autotexts, size=10, weight=\"bold\")\n",
    "    plt.show()\n",
    "\n",
    "label = ['Positif', 'Negatif', 'Netral']\n",
    "count_data = [total_positif+1, total_negatif+1, total_netral]\n",
    "\n",
    "show_pie(label, count_data, \"Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.columns[[0]], axis = 1, inplace = True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('TrainXl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentimen = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = []\n",
    "for index, row in df_sentimen.iterrows():\n",
    "    if row[\"Label\"] == 'positif':\n",
    "        Label.append(1)\n",
    "    else:\n",
    "        Label.append(0)\n",
    "\n",
    "df_sentimen[\"label\"] = Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_1 = df_sentimen[df_sentimen['label']==0].sample(200,replace=True, random_state=0)\n",
    "# s_2 = df_sentimen[df_sentimen['label']==1].sample(200,replace=True, random_state=0)\n",
    "# df_pjj = pd.concat([s_1, s_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pjj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = df_sentimen['tweet']\n",
    "y = df_sentimen['label']\n",
    "print(x)\n",
    "print (y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "vectorizer = TfidfVectorizer() \n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"\\n================= confusion matrix ===================== \")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=========================================== \")\n",
    "# accuracy score\n",
    "print(\"accuracy score hasil prediksi adalah: \")\n",
    "print(accuracy_score(y_test, y_pred) * 100)\n",
    "\n",
    "# precision score\n",
    "print(\"precision score hasil prediksi adalah: \")\n",
    "print(precision_score(y_test, y_pred))\n",
    "\n",
    "# recall score\n",
    "print(\"recall score hasil prediksi adalah: \")\n",
    "print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tes uji data baru\n",
    "tweet_pjj1 = [\"pjj buat males belajar\"]\n",
    "tweet1 = tweet_pjj1\n",
    "tweet_pjj2 = [\"alhamdulillah nilai dapat bagus semangat pjjnya\"]\n",
    "tweet2 = tweet_pjj2\n",
    "#merubah ke bentuk tfidf\n",
    "tweet_pjj1 = vectorizer.transform(tweet_pjj1)\n",
    "tweet_pjj2 = vectorizer.transform(tweet_pjj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediksi1 =classifier.predict(tweet_pjj1)\n",
    "prediksi2 =classifier.predict(tweet_pjj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prediksi1 == 0:\n",
    "    print(tweet1, \"negatif\")\n",
    "if prediksi1 == 1:\n",
    "    print(tweet1, \"Positif\")   \n",
    "if prediksi2 == 0:\n",
    "    print(tweet2, \"negatif\")\n",
    "if prediksi2 == 1:\n",
    "    print(tweet2, \"Positif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_databaru = pd.read_excel('Dataset/databarubersih.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_databaru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaru = df_databaru['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweettabel = databaru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#merubah ke bentuk number agar bisa di baca algoritma atau ML\n",
    "databaru = vectorizer.transform(databaru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediksidatabaru =classifier.predict(databaru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(databaru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tweettabel, *prediksidatabaru, sep = \"\\n\") \n",
    "df_prediksidata = pd.DataFrame(tweettabel) \n",
    "df_databaru['polarity'] = prediksidatabaru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_databaru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_databaru.to_excel('Dataset/hasilprediksi.xlsx',encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END CODE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # x_test = vectorizer.transform(x_test)\n",
    "# # y_pred = classifier.predict(x_test)\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# print(\"\\n================= confusion matrix ===================== \")\n",
    "# print(cm)\n",
    "# print('================akurasi Naive bayes 20% data tes=====================\\n')\n",
    "# print(accuracy_score(y_test, y_pred))\n",
    "# # print('================Cross Validation=========================')\n",
    "# # scores = cross_val_score(classifier, x_train, y_train, cv=5)\n",
    "# # # Print the accuracy of each fold:\n",
    "# # print(scores)\n",
    "\n",
    "# # # Print the mean accuracy of all 5 folds\n",
    "# # print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_predict\n",
    "# y_crosspredict = cross_val_predict(classifier, x_train, y_train, cv=5)\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# conf_mat = confusion_matrix(y_train, y_crosspredict)\n",
    "# print(conf_mat)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.plot(scores, 'o-')\n",
    "# plt.xlabel('askurasi multinomial naive bayes')\n",
    "# plt.ylabel('Cross-validated accuracy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "teks_training = df_sentimen.iloc[:,0] #:,0 klolom ke 0\n",
    "label_output = df_sentimen.iloc[:,-1] #:, -1 kolomg terakhir"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "label_output.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#memisahkan data training dan testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(teks_training, label_output, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#tes uji data baru\n",
    "tweet_pjj = ['hore pjj udah selesai', 'gara-gara pjj nilai jadi turun']\n",
    "#merubah ke bentuk number agar bisa di baca algoritma atau ML\n",
    "tweet_pjj = vectorizer.transform(tweet_pjj)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prediksi =classifier.predict(tweet_pjj)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(prediksi)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#prediksi data yang sudah ada (labelingdata.xlsx)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sn\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm ,figsize=(7,4))\n",
    "plt.show()\n",
    "# plt.figure(figsize = (10,8))\n",
    "# sn.heatmap(cm, annot=True, al)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('my-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a7bee19b568be8d98125153f5c3b19aa21d2d91c4444c24403043c900e7bd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
